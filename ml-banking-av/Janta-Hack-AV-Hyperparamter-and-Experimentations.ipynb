{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/jantahack/train_fNxu4vz.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/jantahack/test_fjtUOL8.csv\")\n",
    "subm = pd.read_csv(\"/kaggle/input/jantahack/sample_submission_HSqiq1Q.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train.copy()\n",
    "test_df = test.copy()\n",
    "joined_df = pd.concat([train_df,test_df],axis=0)\n",
    "joined_df = joined_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Pre-processed Data\n",
    "joined_df = pd.read_csv(\"../input/preprocessed/preprocess/combined_basic.csv\")\n",
    "\n",
    "\n",
    "## Lets separate train and test and Label Encode and dummy encode and try which models best\n",
    "train_df = joined_df[:164309]\n",
    "test_df = joined_df[164309:]\n",
    "train_df.drop(\"Loan_ID\",axis=1,inplace=True)\n",
    "test_df.drop([\"Loan_ID\",\"Interest_Rate\"],axis=1,inplace=True)\n",
    "print(train_df.shape,test_df.shape)\n",
    "\n",
    "X_train,y_train = train_df.drop(\"Interest_Rate\",1),train_df.loc[:,\"Interest_Rate\"]\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGboost baseline model and its params\n",
    "xgb_obj = XGBClassifier(\n",
    "                learning_rate=0.300000012,\n",
    "                n_estimators=100,\n",
    "                max_depth=6,\n",
    "                min_child_weight=1,\n",
    "                gamma=0, \n",
    "                subsample=1,\n",
    "                colsample_bytree=1,\n",
    "                objective='multi:softprob', \n",
    "                random_state=21, n_jobs=-1,\n",
    "                scale_pos_weight=None, \n",
    "                verbosity=3,\n",
    "                seed=21\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Max Depth and min_child_wt\n",
    "- As these are the parameters which will affect XGBoost the most as they control the tree structure.Hence tuning them first.Leaving other parameters as default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tunings the above findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    " 'max_depth':[5,6,7],\n",
    " 'min_child_weight':[6,8,10,12,14]\n",
    "}\n",
    "\n",
    "model = XGBClassifier(\n",
    "              colsample_bytree=0.8, gamma=0,learning_rate=0.300000012, max_depth=6,\n",
    "              min_child_weight=1,n_estimators=100, n_jobs=-1,objective='multi:softprob', \n",
    "              subsample=0.8,verbosity=3,seed=21)\n",
    "\n",
    "\n",
    "gsearch = GridSearchCV(estimator = model, param_grid = params, scoring='f1_weighted',n_jobs=-1, cv=4,return_train_score=True,verbose=7)\n",
    "\n",
    "gsearch.fit(X_train,y_train)\n",
    "print(f'Best scor = {gsearch.best_score_} and fixing the params as {gsearch.best_params_}')\n",
    "params = gsearch.cv_results_[\"params\"]\n",
    "test_f1= gsearch.cv_results_[\"mean_test_score\"]\n",
    "train_f1 = gsearch.cv_results_[\"mean_train_score\"]\n",
    "df = pd.DataFrame({\"xaxis\":params,\"train\":train_f1,\"test\":test_f1})\n",
    "df = df.sort_values(by=\"test\",ascending=False).reset_index(drop=True)\n",
    "df[:10]\n",
    "\n",
    "## Best scor = 0.5315695946969097 and fixing the params as {'max_depth': 6, 'min_child_weight': 14}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "\n",
    "xgb_obj = XGBClassifier(\n",
    "                learning_rate=0.300000012,\n",
    "                n_estimators=100,\n",
    "                subsample=1,\n",
    "                colsample_bytree=1,\n",
    "                objective='multi:softprob', \n",
    "                random_state=21, n_jobs=-1,\n",
    "                scale_pos_weight=None, \n",
    "                verbosity=3,\n",
    "                max_depth=6,\n",
    "                min_child_weight =14,\n",
    "                seed=21\n",
    "              )\n",
    "\n",
    "rscv = RandomizedSearchCV(xgb_obj,params,random_state=21,cv=4,verbose=4,n_jobs=-1,return_train_score=True,scoring='f1_weighted')\n",
    "rscv.fit(X_train,y_train)\n",
    "\n",
    "x = [i/10.0 for i in range(0,5)]\n",
    "test_f1= rscv.cv_results_[\"mean_test_score\"]\n",
    "train_f1 = rscv.cv_results_[\"mean_train_score\"]\n",
    "print(f'Best scor = {rscv.best_score_} and fixing the params as {rscv.best_params_}')\n",
    "\n",
    "df = pd.DataFrame({\"xaxis\":x,\"train\":train_f1,\"test\":test_f1})\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[\"xaxis\"], y=df[\"train\"],\n",
    "                    mode='lines+markers',\n",
    "                    name='train_mse'))\n",
    "fig.add_trace(go.Scatter(x=df[\"xaxis\"], y=df[\"test\"],\n",
    "                    mode='lines+markers',\n",
    "                    name='test_mse'))\n",
    "fig.show()\n",
    "\n",
    "## Best scor = 0.531569862841758 and fixing the params as {'gamma': 0.3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Subasmple and colsample by tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "model = XGBClassifier( \n",
    "              colsample_bytree=0.8, gamma=0,learning_rate=0.300000012, max_depth=6,\n",
    "              min_child_weight=14,n_estimators=100, n_jobs=-1,objective='multi:softprob', \n",
    "              subsample=0.8,verbosity=3 scale_pos_weight=1,seed=21)\n",
    "\n",
    "\n",
    "gsearch = GridSearchCV(estimator = model, param_grid = params, scoring='f1_weighted',n_jobs=-1, cv=4,return_train_score=True,verbose=7)\n",
    "gsearch.fit(X_train,y_train)\n",
    "\n",
    "print(f'Best scor = {gsearch.best_score_} and fixing the params as {gsearch.best_params_}')\n",
    "params = gsearch.cv_results_[\"params\"]\n",
    "test_f1= gsearch.cv_results_[\"mean_test_score\"]\n",
    "train_f1 = gsearch.cv_results_[\"mean_train_score\"]\n",
    "df = pd.DataFrame({\"xaxis\":params,\"train\":train_f1,\"test\":test_f1})\n",
    "df = df.sort_values(by=\"test\",ascending=False).reset_index(drop=True)\n",
    "df[:10]\n",
    "\n",
    "\n",
    "##  Here after finding a value they are trying more precise values i.e fine tuning\n",
    "\n",
    "# Here, we found 0.8 as the optimum value for both subsample and colsample_bytree. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    " 'learning_rate':[0.150000006,0.05,0.5,0.1,0.2],\n",
    "  'n_estimators':[200,600,60,300,150]\n",
    "}\n",
    "\n",
    "xgb_obj = XGBClassifier(\n",
    "                subsample=1,\n",
    "                colsample_bytree=1,\n",
    "                objective='multi:softprob', \n",
    "                random_state=21, n_jobs=-1,\n",
    "                scale_pos_weight=None, \n",
    "                verbosity=3,\n",
    "                max_depth=6,\n",
    "                min_child_weight =14,\n",
    "                gamma = 0.3,seed=21\n",
    "              )\n",
    "\n",
    "rscv = RandomizedSearchCV(xgb_obj,params,random_state=21,cv=4,verbose=4,n_jobs=-1,return_train_score=True,scoring='f1_weighted')\n",
    "rscv.fit(X_train,y_train)\n",
    "\n",
    "# lr = 0.2\n",
    "# n_estimator=150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data-loan-cat/train_fNxu4vz.csv\")\n",
    "test = pd.read_csv(\"data-loan-cat/test_fjtUOL8.csv\")\n",
    "subm = pd.read_csv(\"data-loan-cat/sample_submission_HSqiq1Q.csv\")\n",
    "\n",
    "\n",
    "train_df,test_df = train.copy(),test.copy()\n",
    "joined_df = pd.concat([train_df,test_df],axis=0)\n",
    "joined_df.reset_index(drop=True,inplace=True)\n",
    "print(f'OG Train shape ={train_df.shape}\\nOG Test shape ={test_df.shape}\\nOG Join shape ={joined_df.shape}')\n",
    "\n",
    "## Coverting to int \n",
    "joined_df[\"Loan_Amount_Requested\"]= joined_df[\"Loan_Amount_Requested\"].str.replace('\\D+','').astype(int)\n",
    "\n",
    "\n",
    "## Numerical and cat cols\n",
    "numerical = [col for col in joined_df.columns if joined_df[col].dtype!='object']\n",
    "categorical = [col for col in joined_df.columns if joined_df[col].dtype =='object']\n",
    "# print(*numerical,sep=\" \")\n",
    "# print(\"--------------------------------------------\\n\")\n",
    "# print(*categorical,sep=\" \")\n",
    "\n",
    "\n",
    "# ## Imuting missing values\n",
    "joined_df[\"Length_Employed\"].fillna(joined_df[\"Length_Employed\"].mode()[0],inplace=True)\n",
    "joined_df[\"Home_Owner\"].fillna(joined_df[\"Home_Owner\"].mode()[0],inplace=True)\n",
    "joined_df[\"Annual_Income\"].fillna(joined_df[\"Annual_Income\"].median(),inplace=True)\n",
    "joined_df[\"Months_Since_Deliquency\"].fillna(joined_df[\"Months_Since_Deliquency\"].median(),inplace=True)\n",
    "\n",
    "\n",
    "## Dropping Id\n",
    "joined_df.drop(\"Loan_ID\",axis=1,inplace=True)\n",
    "print(f'Shape after dropping loan id {joined_df.shape}')\n",
    "\n",
    "## Binning  length employeed\n",
    "mapping = {'< 1 year':\"less_than4\",\n",
    "           '2 years ':\"less_than4\",\n",
    "           '3 years ':\"less_than4\",\n",
    "           '4 years':\"between_4to8\",\n",
    "           '5 years':\"between_4to8\",\n",
    "           '6 years' : \"between_4to8\",\n",
    "           '7 years':\"between_4to8\",\n",
    "           '8 years':'greater_than8',\n",
    "           \"9 years\":'greater_than8',\n",
    "           \"10+ years\":'greater_than8'\n",
    "    \n",
    "}\n",
    "\n",
    "joined_df['Length_Employed'] = joined_df['Length_Employed'].map(mapping)\n",
    "print(joined_df[\"Length_Employed\"].value_counts())\n",
    "\n",
    "\n",
    "## Binning home owner\n",
    "mapping = {'Mortgage':\"Mortgage\",\n",
    "           'Rent ':\"Rent\",\n",
    "           'Own':\"Own\",\n",
    "           'Other':\"Other\",\n",
    "           'None':\"Other\"\n",
    "           }\n",
    "\n",
    "joined_df['Home_Owner'] = joined_df['Home_Owner'].map(mapping)\n",
    "print(joined_df[\"Home_Owner\"].value_counts())\n",
    "\n",
    "## One Hot ENcode\n",
    "joined_df = pd.concat([ \n",
    "            joined_df.select_dtypes(exclude='object'),\n",
    "            pd.get_dummies(joined_df['Length_Employed'],drop_first = True),\n",
    "            pd.get_dummies(joined_df['Home_Owner'],drop_first = True),\n",
    "            pd.get_dummies(joined_df['Income_Verified'],drop_first = True),\n",
    "            pd.get_dummies(joined_df['Purpose_Of_Loan'],drop_first = True),\n",
    "            pd.get_dummies(joined_df['Gender'],drop_first = True)\n",
    "            \n",
    "            ],axis=1)\n",
    "\n",
    "print(f'Shape after One HOt encode {joined_df.shape}')\n",
    "joined_df.to_csv(\"combined_only_binning.csv\",index=False)\n",
    "print(\"Only binned saved !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def create_ploynomial_interactions(df):\n",
    "    print(f'Old shape of the df {df.shape}\\n')\n",
    "    \n",
    "    combos = list(combinations(list(df.columns), 2))\n",
    "    print(f'combination  = {len(combos)}\\n')\n",
    "    \n",
    "    colnames = list(df.columns)+['_'.join(x) for x in combos]\n",
    "    print(f'Total columns now = {len(colnames)}\\n')\n",
    "    \n",
    "    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "    df = poly.fit_transform(df)\n",
    "    print(f'New shape of the df after creating new polynomial features = {df.shape}\\n')\n",
    "    df = pd.DataFrame(df)\n",
    "    df.columns = colnames\n",
    "    \n",
    "    ## Dropping the columns which contains all zero as their values i.e column filled with zeroes\n",
    "    noint_indices = [col_name for col_name,contains_zero in enumerate(list((df==0).all())) if contains_zero]\n",
    "    df= df.drop(df.columns[noint_indices], axis=1)\n",
    "    print(f'Final shape of the df after removing zero columns = {df.shape}\\n')\n",
    "\n",
    "    \n",
    "    return df,colnames\n",
    "\n",
    "X = joined_df.drop(\"Interest_Rate\",1)\n",
    "t = X.copy()\n",
    "y = joined_df.loc[:,\"Interest_Rate\"]\n",
    "X,old_cols= create_ploynomial_interactions(X)\n",
    "print(X.shape)\n",
    "\n",
    "joined_df = pd.concat([X,y],axis=1)\n",
    "print(f'Final join shape {joined_df.shape}')\n",
    "joined_df.to_csv(\"binning_poly.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's separate train and test \n",
    "train_df = joined_df[:164309]\n",
    "test_df = joined_df[164309:]\n",
    "\n",
    "test_df.reset_index(drop=True,inplace=True) # since indexes were not starting from zero\n",
    "test_df.drop([\"Interest_Rate\"],axis=1,inplace=True)\n",
    "print(f'Final Train shape {train_df.shape} \\nFinal Test Shape {test_df.shape}')\n",
    "\n",
    "## Polynomial Dataset\n",
    "X_train,y_train = train_df.drop(\"Interest_Rate\",1),train_df.loc[:,\"Interest_Rate\"]\n",
    "print(f'Training shape {X_train.shape,y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only Binned Dataset\n",
    "\n",
    "only_binned = pd.read_csv(\"combined_only_binning.csv\")\n",
    "only_binned_train = only_binned[:164309]\n",
    "\n",
    "only_binned_test = only_binned[164309:]\n",
    "only_binned_test.reset_index(drop=True,inplace=True)\n",
    "only_binned_test.drop(\"Interest_Rate\",axis=1,inplace=True)\n",
    "\n",
    "ob_Xtrain , ob_ytrain = only_binned_train.drop(\"Interest_Rate\",1),only_binned_train.loc[:,\"Interest_Rate\"]\n",
    "final_Xtrain = pd.concat([ob_Xtrain,X_train[columns]],axis=1)\n",
    "final_Xtest = pd.concat([only_binned_test,test_df[columns]],axis=1)\n",
    "\n",
    "print(ob_Xtrain.shape[1]+100,only_binned_test.shape[1]+100)\n",
    "print(final_Xtrain.shape,final_Xtest.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Modelling\n",
    "## feature_names must be unique Due to duplicate columns\n",
    "final_Xtrain = final_Xtrain.loc[:,~final_Xtrain.columns.duplicated()]\n",
    "final_Xtest = final_Xtest.loc[:,~final_Xtest.columns.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Loan_Amount_Requested_Annual_Income',\n",
    " 'Loan_Amount_Requested_Months_Since_Deliquency',\n",
    " 'Loan_Amount_Requested_Inquiries_Last_6Mo',\n",
    " 'Loan_Amount_Requested_Debt_To_Income',\n",
    " 'Loan_Amount_Requested_Number_Open_Accounts',\n",
    " 'vacation',\n",
    " 'Loan_Amount_Requested_greater_than8',\n",
    " 'Loan_Amount_Requested_Own',\n",
    " 'Loan_Amount_Requested_house',\n",
    " 'Loan_Amount_Requested_less_than4',\n",
    " 'Loan_Amount_Requested_Other',\n",
    " 'Loan_Amount_Requested_Total_Accounts',\n",
    " 'major_purchase',\n",
    " 'Months_Since_Deliquency',\n",
    " 'Loan_Amount_Requested_debt_consolidation',\n",
    " 'Loan_Amount_Requested',\n",
    " 'Loan_Amount_Requested_credit_card',\n",
    " 'Loan_Amount_Requested_Male',\n",
    " 'Annual_Income_moving',\n",
    " 'Loan_Amount_Requested_home_improvement',\n",
    " 'Loan_Amount_Requested_educational',\n",
    " 'Loan_Amount_Requested_not verified',\n",
    " 'Annual_Income_Total_Accounts',\n",
    " 'Loan_Amount_Requested_VERIFIED - income source',\n",
    " 'Loan_Amount_Requested_medical',\n",
    " 'Loan_Amount_Requested_moving',\n",
    " 'Annual_Income_Other',\n",
    " 'Loan_Amount_Requested_renewable_energy',\n",
    " 'Annual_Income_Debt_To_Income',\n",
    " 'Annual_Income_less_than4',\n",
    " 'Loan_Amount_Requested_vacation',\n",
    " 'Annual_Income_greater_than8',\n",
    " 'Annual_Income_not verified',\n",
    " 'Annual_Income_Number_Open_Accounts',\n",
    " 'Annual_Income_Months_Since_Deliquency',\n",
    " 'Loan_Amount_Requested_small_business',\n",
    " 'Annual_Income_Own',\n",
    " 'Annual_Income_Inquiries_Last_6Mo',\n",
    " 'Loan_Amount_Requested_other',\n",
    " 'Loan_Amount_Requested_major_purchase',\n",
    " 'Annual_Income_educational',\n",
    " 'Annual_Income_home_improvement',\n",
    " 'Annual_Income_VERIFIED - income source',\n",
    " 'Debt_To_Income',\n",
    " 'home_improvement',\n",
    " 'Annual_Income_house',\n",
    " 'Annual_Income_credit_card',\n",
    " 'Annual_Income',\n",
    " 'Annual_Income_debt_consolidation',\n",
    " 'wedding',\n",
    " 'Annual_Income_medical',\n",
    " 'educational',\n",
    " 'house',\n",
    " 'Total_Accounts',\n",
    " 'medical',\n",
    " 'Number_Open_Accounts',\n",
    " 'Loan_Amount_Requested_wedding',\n",
    " 'less_than4',\n",
    " 'not verified',\n",
    " 'Inquiries_Last_6Mo',\n",
    " 'Other',\n",
    " 'Annual_Income_major_purchase',\n",
    " 'renewable_energy',\n",
    " 'greater_than8',\n",
    " 'moving',\n",
    " 'Own',\n",
    " 'Male',\n",
    " 'debt_consolidation',\n",
    " 'credit_card',\n",
    " 'VERIFIED - income source',\n",
    " 'other',\n",
    " 'small_business']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "                learning_rate=0.2,\n",
    "                n_estimators=150,\n",
    "                subsample=1,\n",
    "                colsample_bytree=1,\n",
    "                objective='multi:softprob', \n",
    "                n_jobs=-1,\n",
    "                scale_pos_weight=None, \n",
    "                verbosity=3,\n",
    "                max_depth=6,\n",
    "                min_child_weight =14,\n",
    "                gamma = 0.3\n",
    "               \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_evaluate(model,X,y,cv,scoring,verbose,model_name):\n",
    "    weighted_f1s = cross_val_score(model,X,y,cv=cv,scoring=scoring,verbose=verbose,n_jobs=-1)\n",
    "    mean_weighted_f1 = round(np.sum(weighted_f1s)/cv,5)\n",
    "    print(f\" -----------------------{model_name}-------------------------------\")\n",
    "    print(f\" weightedF1 for folds = {weighted_f1s}\\n And Mean weighted_f1 on cv = {mean_weighted_f1}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top 60\n",
    "top60=columns[:60].copy()\n",
    "\n",
    "final_Xtrain = pd.concat([ob_Xtrain,X_train[top60]],axis=1)\n",
    "final_Xtest = pd.concat([only_binned_test,test_df[top60]],axis=1)\n",
    "\n",
    "final_Xtrain = final_Xtrain.loc[:,~final_Xtrain.columns.duplicated()]\n",
    "final_Xtest = final_Xtest.loc[:,~final_Xtest.columns.duplicated()]\n",
    "\n",
    "xgb60 = XGBClassifier(verbosity=3,random_state=21)\n",
    "cross_val_evaluate(xgb60,final_Xtrain,y_train,3,'f1_weighted',4,\"XGB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top 50\n",
    "top50=columns[:50].copy()\n",
    "\n",
    "\n",
    "final_Xtrain = pd.concat([ob_Xtrain,X_train[top50]],axis=1)\n",
    "final_Xtest = pd.concat([only_binned_test,test_df[top50]],axis=1)\n",
    "\n",
    "final_Xtrain = final_Xtrain.loc[:,~final_Xtrain.columns.duplicated()]\n",
    "final_Xtest = final_Xtest.loc[:,~final_Xtest.columns.duplicated()]\n",
    "\n",
    "xgb50 = XGBClassifier(verbosity=3,random_state=21)\n",
    "cross_val_evaluate(xgb50,final_Xtrain,y_train,3,'f1_weighted',4,\"XGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top 40\n",
    "top40=columns[:40].copy()\n",
    "\n",
    "\n",
    "final_Xtrain = pd.concat([ob_Xtrain,X_train[top40]],axis=1)\n",
    "final_Xtest = pd.concat([only_binned_test,test_df[top40]],axis=1)\n",
    "\n",
    "final_Xtrain = final_Xtrain.loc[:,~final_Xtrain.columns.duplicated()]\n",
    "final_Xtest = final_Xtest.loc[:,~final_Xtest.columns.duplicated()]\n",
    "\n",
    "xgb40 = XGBClassifier(verbosity=3,random_state=21)\n",
    "cross_val_evaluate(xgb40,final_Xtrain,y_train,3,'f1_weighted',4,\"XGB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
